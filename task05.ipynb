{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5ce6c0e-deee-4404-b5cc-da2901e5d5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749c3cd3-b1c5-43b5-b588-65a3e8c4c132",
   "metadata": {},
   "source": [
    "# 5.1 pytorch模型定义"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64836fc3-0f1d-4638-9231-848c11e9065c",
   "metadata": {},
   "source": [
    "## 1. Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1c008d-c251-461d-8054-b3d47fd66105",
   "metadata": {},
   "source": [
    "### （1）md文件样例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "828833f6-6263-46a3-b6e3-7b5a89ec2fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n",
      "Sequential(\n",
      "  (fc1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc2): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "class MySequential(nn.Module):\n",
    "    from collections import OrderedDict\n",
    "    def __init__(self, *args):\n",
    "        super(MySequential, self).__init__()\n",
    "        if len(args) == 1 and isinstance(args[0], OrderedDict): # 如果传入的是一个OrderedDict\n",
    "            for key, module in args[0].items():\n",
    "                self.add_module(key, module)  # add_module方法会将module添加进self._modules(一个OrderedDict)\n",
    "        else:  # 传入的是一些Module\n",
    "            for idx, module in enumerate(args):\n",
    "                self.add_module(str(idx), module)\n",
    "    def forward(self, input):\n",
    "        # self._modules返回一个 OrderedDict，保证会按照成员添加时的顺序遍历成\n",
    "        for module in self._modules.values():\n",
    "            input = module(input)\n",
    "        return input\n",
    "\n",
    "# 直接排列\n",
    "net = nn.Sequential(\n",
    "        nn.Linear(784, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 10),\n",
    "        )\n",
    "print(net)\n",
    "# 使用OrderedDict\n",
    "import collections\n",
    "import torch.nn as nn\n",
    "net2 = nn.Sequential(collections.OrderedDict([\n",
    "          ('fc1', nn.Linear(784, 256)),\n",
    "          ('relu1', nn.ReLU()),\n",
    "          ('fc2', nn.Linear(256, 10))\n",
    "          ]))\n",
    "print(net2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea127cbb-af38-4f68-aa99-f5ca18cf6784",
   "metadata": {},
   "source": [
    "### （2）test1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1d1b32-7b4c-464b-9c3e-ecb28f5ad41a",
   "metadata": {},
   "source": [
    "Sequential实现了内部的 forward 函数，而且模块必须是按照顺序进行排列的，所以必须确保前一个模块的输出大小和下一个模块的输入大小是一致的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d107a13-e39c-4ec6-8da7-2f13549f3c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net1_1(\n",
      "  (block): Sequential(\n",
      "    (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (3): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class net1_1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net1_1, self).__init__()\n",
    "        self.block = nn.Sequential(nn.Conv2d(1,20,5),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Conv2d(20,64,5),\n",
    "                                    nn.ReLU())\n",
    "    def forward(self, x):\n",
    "        x = self.block(x)\n",
    "        return x\n",
    "\n",
    "net = net1_1()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2571103-799b-4ef6-ac66-b0fcfc6a7402",
   "metadata": {},
   "source": [
    "## 2. ModuleList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3164024d-36b2-4ed5-876c-d3ecda213010",
   "metadata": {},
   "source": [
    "### （1）md样例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a77b0a0-621a-411e-aea9-cdb680e5f18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=256, out_features=10, bias=True)\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class model(nn.Module):\n",
    "  def __init__(self, *args):\n",
    "        super(MySequential, self).__init__()\n",
    "        if len(args) == 1 and isinstance(args[0], OrderedDict): # 如果传入的是一个OrderedDict\n",
    "            for key, module in args[0].items():\n",
    "                self.add_module(key, module)  # add_module方法会将module添加进self._modules(一个OrderedDict)\n",
    "        else:  # 传入的是一些Module\n",
    "            for idx, module in enumerate(args):\n",
    "                self.add_module(str(idx), module)\n",
    "  def forward(self, x):\n",
    "    for layer in self.modulelist:\n",
    "      x = layer(x)\n",
    "    return x\n",
    "\n",
    "net = nn.ModuleList([nn.Linear(784, 256), nn.ReLU()])\n",
    "net.append(nn.Linear(256, 10)) # # 类似List的append操作\n",
    "print(net[-1])  # 类似List的索引访问\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53954b8a-5402-4f21-a16f-94016a9a2bd1",
   "metadata": {},
   "source": [
    "### （2）test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6200766-e4e0-49fc-96f5-2bd85307a9b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[nn.Linear(10,10) for i in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abe2a95d-933e-4d7a-a89c-1fc06cf55186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net1(\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (1): Linear(in_features=10, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "<class 'torch.Tensor'> torch.Size([10, 10])\n",
      "<class 'torch.Tensor'> torch.Size([10])\n",
      "<class 'torch.Tensor'> torch.Size([10, 10])\n",
      "<class 'torch.Tensor'> torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "class net1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net1, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(10,10) for i in range(2)])\n",
    "    def forward(self, x):\n",
    "        for m in self.linears:\n",
    "            x = m(x)\n",
    "        return x\n",
    "\n",
    "net = net1()\n",
    "print(net)\n",
    "for param in net.parameters():\n",
    "    print(type(param.data), param.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb9db94-a94d-43c9-b9db-23f8a2cf3d4c",
   "metadata": {},
   "source": [
    "包含两个全连接层，他们的权重 (weithgs) 和偏置 (bias) 都在这个网络之内"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c73d8b0-8899-4735-9df9-b8f18ef1b102",
   "metadata": {},
   "source": [
    "### （3）test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5467ce44-f4b4-4380-a488-f949a741c22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net2, self).__init__()\n",
    "        self.linears = [nn.Linear(10,10) for i in range(2)]\n",
    "    def forward(self, x):\n",
    "        for m in self.linears:\n",
    "            x = m(x)\n",
    "        return x\n",
    "\n",
    "net = net2()\n",
    "print(net)\n",
    "print(list(net.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57db5c1b-2e7e-4845-9e8e-878454a0a80f",
   "metadata": {},
   "source": [
    "该网络借助python自带的list来添加全连接层和 parameters ，但是并没有自动注册到网络中。但可以使用 forward 来计算输出结果。但进行实例化网络进行训练的时候，这些层的 parameters 不在整个网络之中，所以其网络参数也不会被更新，即无法训练。\n",
    "\n",
    "nn.ModuleList 是一个储存不同 module，并自动将每个 module 的 parameters 添加到网络之中的容器。但是，其并未定义一个网络，它只是将不同的模块储存在一起，这些模块之间并没有什么先后顺序可言。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69047174-1a9b-4510-8739-ecebc1002211",
   "metadata": {},
   "source": [
    "### （4）test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "881aa464-99b6-454d-93af-a0a081ebc3aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Linear(in_features=10, out_features=20, bias=True),\n",
       " Linear(in_features=20, out_features=30, bias=True),\n",
       " Linear(in_features=5, out_features=10, bias=True)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[nn.Linear(10,20), nn.Linear(20,30), nn.Linear(5,10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ad0e653-c5ba-44d3-8852-a1b99404e003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net3(\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=10, out_features=20, bias=True)\n",
      "    (1): Linear(in_features=20, out_features=30, bias=True)\n",
      "    (2): Linear(in_features=5, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "torch.Size([32, 30])\n"
     ]
    }
   ],
   "source": [
    "class net3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net3, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(10,20), nn.Linear(20,30), nn.Linear(5,10)])\n",
    "    def forward(self, x):\n",
    "        x = self.linears[2](x)\n",
    "        x = self.linears[0](x)\n",
    "        x = self.linears[1](x) \n",
    "        return x\n",
    "\n",
    "net = net3()\n",
    "print(net)\n",
    "input = torch.randn(32, 5)\n",
    "print(net(input).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4f9dcc-0c6f-43bf-99c7-d9531965396e",
   "metadata": {},
   "source": [
    "可以看出ModuleList 里面的顺序并不能决定什么，网络的执行顺序是根据 forward 函数来决定的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21a1617b-ceb4-4e00-aff5-266d5f25a930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0583, -0.1632, -0.4806,  0.1888, -0.0617],\n",
       "        [ 0.3558,  0.6158, -0.8599,  1.1136, -0.2691],\n",
       "        [ 0.1366,  2.1078,  0.0322,  0.9173, -0.7663],\n",
       "        [-0.2109, -1.1150,  0.3461,  0.5797, -0.0623],\n",
       "        [ 0.4662, -0.2846,  1.5440, -0.7857,  0.7775],\n",
       "        [-0.4085,  0.5001,  0.7159, -0.9582,  1.1342],\n",
       "        [ 0.7358, -0.0676,  0.4093,  0.6470, -1.2525],\n",
       "        [ 0.4209, -1.1493, -2.5390, -0.5123,  0.1896],\n",
       "        [-0.1026,  0.6707, -1.5462, -1.3886,  0.8138],\n",
       "        [-0.3662, -1.0210,  0.1277, -1.4965,  0.4556],\n",
       "        [ 0.2223, -0.3035, -0.2468, -0.5720, -0.2104],\n",
       "        [-0.8054,  1.6651,  1.6164,  0.9365, -0.2867],\n",
       "        [ 1.0655, -0.8571, -0.3738,  0.4813,  1.0100],\n",
       "        [-0.2176,  1.1231,  0.7701,  1.8725, -0.0440],\n",
       "        [-0.6627, -1.3341,  0.2391, -0.1013, -2.0197],\n",
       "        [ 2.3212,  0.5123,  0.0277,  0.4985,  0.9116],\n",
       "        [ 0.3701, -1.0235, -0.9469, -0.2769,  1.3290],\n",
       "        [-0.1712, -1.5453,  0.6391,  1.4576,  0.0420],\n",
       "        [-0.2927,  0.6904, -0.1627, -3.0780,  1.6508],\n",
       "        [ 1.0311, -1.3576,  0.2097,  0.2396,  0.6056],\n",
       "        [-0.0826,  1.1895,  0.5474, -0.5470, -0.2749],\n",
       "        [-1.2617,  0.1395,  2.5348, -0.9098, -0.0463],\n",
       "        [-0.9298,  1.4273, -1.1622, -1.3234, -0.2513],\n",
       "        [ 0.2458, -1.1479, -3.1648,  0.9193, -0.4439],\n",
       "        [-2.0269, -0.6822,  1.7015, -0.0573,  1.4514],\n",
       "        [ 0.2157,  1.6405, -1.6878,  0.7395, -0.0736],\n",
       "        [-0.1791, -0.2560, -0.6918,  0.6330, -0.8756],\n",
       "        [-1.5222, -0.8351,  0.3896, -0.8220, -1.0542],\n",
       "        [ 0.8558, -0.7774, -1.7506,  1.4747,  0.3508],\n",
       "        [ 1.3034,  0.5075,  0.1392, -2.0827, -2.3977],\n",
       "        [ 1.5441, -0.1829, -0.8704,  1.5416,  0.8552],\n",
       "        [ 1.0089,  0.2796, -1.2930, -0.6114,  0.8037]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408b4bbe-5bd2-4f70-9b60-b9bda18ed150",
   "metadata": {},
   "source": [
    "### （5）test4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742ecbee-519c-4d2d-90c5-32a18cae43a1",
   "metadata": {},
   "source": [
    "一个模块也可以在 forward 函数中被调用多次。但是，被调用多次的模块，无论之后怎么更新，都使用同一组 parameters 的，即它们的参数是共享的，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6eca52b2-3ba3-41ae-88c7-510bdd2be464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net4(\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=5, out_features=10, bias=True)\n",
      "    (1): Linear(in_features=10, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "linears.0.weight torch.Size([10, 5])\n",
      "linears.0.bias torch.Size([10])\n",
      "linears.1.weight torch.Size([10, 10])\n",
      "linears.1.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "class net4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net4, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(5, 10), nn.Linear(10, 10)])\n",
    "    def forward(self, x):\n",
    "        x = self.linears[0](x)\n",
    "        x = self.linears[1](x)\n",
    "        x = self.linears[1](x)\n",
    "        return x\n",
    "\n",
    "net = net4()\n",
    "print(net)\n",
    "for name, param in net.named_parameters():\n",
    "    print(name, param.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4202f09e-68f8-4b07-b5f3-45d78a5d183e",
   "metadata": {},
   "source": [
    "## 3. ModuleDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6d9b2ff-9172-4352-bede-cfd187099b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=784, out_features=256, bias=True)\n",
      "Linear(in_features=256, out_features=10, bias=True)\n",
      "ModuleDict(\n",
      "  (linear): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (act): ReLU()\n",
      "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = nn.ModuleDict({\n",
    "    'linear': nn.Linear(784, 256),\n",
    "    'act': nn.ReLU(),\n",
    "})\n",
    "net['output'] = nn.Linear(256, 10) # 添加\n",
    "print(net['linear']) # 访问\n",
    "print(net.output)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfa72ca-9a26-4ac8-814e-6c71db7c5f84",
   "metadata": {},
   "source": [
    "## 4. 定义方式比较"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2591a2b0-a42c-4042-b2a1-20b013af81e9",
   "metadata": {},
   "source": [
    "### 场景一"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9e7c55-eb17-45d0-b752-4f79957d621c",
   "metadata": {},
   "source": [
    "网络中有很多相似或者重复的层,用 for 循环来创建它们。此时借助 ModuleList 定义方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80a97d6e-93f3-4af0-af2c-b80823489c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [nn.Linear(10, 10) for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "761718e0-08b3-4e9f-a519-994158819e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net5(\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (1): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class net5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net5, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(3)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.linears:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "net = net5()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43edec27-f16a-4c68-9d95-2873d434fc3c",
   "metadata": {},
   "source": [
    "也可以用 Sequential 来实现，但要注意 * 这个操作符，它可以把一个 list 拆开成一个个独立的元素。但是，请注意这个 list 里面的模块必须是按照想要的顺序来进行排列的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bf4b7e2-ee46-4d01-8f71-473563041c6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'net6' object has no attribute 'linears_list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-63dfcabb35fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet6\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-63dfcabb35fa>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinears\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinears_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\software\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1183\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1184\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1185\u001b[1;33m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[0;32m   1186\u001b[0m             type(self).__name__, name))\n\u001b[0;32m   1187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'net6' object has no attribute 'linears_list'"
     ]
    }
   ],
   "source": [
    "class net6(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net6, self).__init__()\n",
    "        self.linear_list = [nn.Linear(10, 10) for i in range(3)]\n",
    "        self.linears = nn.Sequential(*self.linears_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = self.linears(x)\n",
    "        return x\n",
    "\n",
    "net = net6()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b8a864-d9de-4bde-b9e3-dc9e3cd9c2aa",
   "metadata": {},
   "source": [
    "### 场景二"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771ead7d-aba8-4511-b5ba-2a3ba589281f",
   "metadata": {},
   "source": [
    "当我们需要之前层的信息的时候，比如 ResNets 中的 shortcut 结构，或者是像 FCN 中用到的 skip architecture 之类的，当前层的结果需要和之前层中的结果进行融合，一般使用 ModuleList 比较方便，一个非常简单的例子如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51b90bb9-7ae1-44da-bdf9-96e3cfa75df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "119ece49-fdaa-4333-ac5d-daf71a229378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 20])\n",
      "torch.Size([32, 30])\n",
      "torch.Size([32, 50])\n"
     ]
    }
   ],
   "source": [
    "class net7(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net7, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(10, 20), nn.Linear(20, 30), nn.Linear(30, 50)])\n",
    "        self.trace = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.linears:\n",
    "            x = layer(x)\n",
    "            self.trace.append(x)\n",
    "        return x\n",
    "\n",
    "net = net7()\n",
    "input  = torch.randn(32, 10) # input batch size: 32\n",
    "output = net(input)\n",
    "for each in net.trace:\n",
    "    print(each.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48d0132-675a-4378-981d-bb085a9e5de8",
   "metadata": {},
   "source": [
    "使用 trace 的列表来储存网络每层的输出结果，方便后面的层调用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6e7797-2235-441a-ab81-de6413a839a5",
   "metadata": {},
   "source": [
    "# 5.2 快速搭建复杂网络"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a420f0-903c-4eb5-a150-2a47214ff3ca",
   "metadata": {},
   "source": [
    "## 1. Unet模型块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2c51186-c459-44f6-9f46-d3d3cfe6b5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd92ee24-17ee-4c72-8098-8b10543d59d3",
   "metadata": {},
   "source": [
    "## 2. 利用模型块组装U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f1299da-9f59-449b-aa78-eee32a41224d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)  #1\n",
    "        self.down1 = Down(64, 128)  #2\n",
    "        self.down2 = Down(128, 256)  #2\n",
    "        self.down3 = Down(256, 512)  #2\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(512, 1024 // factor) #2\n",
    "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
    "        self.up2 = Up(512, 256 // factor, bilinear)\n",
    "        self.up3 = Up(256, 128 // factor, bilinear)\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb47b12-f522-4fba-90c6-f07511cf48c3",
   "metadata": {},
   "source": [
    "# 5.3 修改模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdef540-f5d5-4ac6-a6bf-9550c450a218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cd343c-8f18-4240-8f96-45bbeb81a49c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74b45a77-a471-48cd-9e7d-c700c63e07a7",
   "metadata": {},
   "source": [
    "# 5.4 模型保存与读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef5bd8b-b5c9-4994-8e8d-400a5072a66b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375d6303-3a3f-4b50-a54d-104858e41067",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cbcc37-159e-4e02-b36e-e5c1716f5ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
